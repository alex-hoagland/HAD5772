---
title: "Lecture 7-8 Code"
author: "Alex Hoagland"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r header}
# I like to include several additional notes in the header of my files here:
#
# Last modified: 2024-02-19 
#
### PURPOSE:
  # Lecture 7-8 code and output file (Regression)
#
### NOTES:
  # - uses the Tidyverse package and Dplyr
library(tidyverse)
library(broom) # helps for storing regression output
library(faux) # Useful package for simulating data
library(modelsummary) # For making regression tables
library(causaldata) # Useful toy data sets

set.seed(122333) # Setting the seed helps to make random number generators give us the same numbers across machines
```

## Regression

### Preliminaries

Let's first set up our data in the case where we **know** our data generating process (DGP). Pretend that we know that

$$ y = 5.5x+u $$

for some random variation $u$.

```{r data-setup}
tb <- tibble(
  x = rnorm(10000),
  u = rnorm(10000),
  y = 5.5*x + 12*u
) # Here's our true model, with some randomness baked in
```

In this case, what happens when we run a regression of $y$ on $x$?

```{r simple-reg}
tb %>% lm(y ~ x, .) %>% summary()

lm(y~x, data=tb) # This is the same as the above command
```

### Visualizing Regressions

There's **a lot** we can do in R with regressions. Let's try some!

```{r model-viz}
# Looking at your model output
reg_tb <- lm(y ~ x, data=tb) # Store regression as an object 

summary(reg_tb) # gives you a full summary
regdata <- broom::tidy(reg_tb, conf.int=TRUE) # Stores regression output in a data frame
regouts <- broom::glance(reg_tb) # Stores other regression features in a data frame
```

Rather than just seeing what the regression returns (e.g., in a table), we can also look at its performance in the data (e.g., with more data visualizations)

```{r reg-performance}
tb <- tb %>% 
  mutate(
    yhat1 = predict(lm(y ~ x, .)),
    yhat2 = 0.0732608 + 5.685033*x, 
    uhat1 = residuals(lm(y ~ x, .)),
    uhat2 = y - yhat2
  ) # How close are our predictions? 

summary(tb[-1:-3])
```

The most common visualization, especially for a one-dimensional regression, is just to plot the line against the data. Look closely at this command and let's walk through the syntax as we develop the plot.

```{r reg-viz-2}
tb %>% 
  lm(y ~ x, .) %>% 
  ggplot(aes(x=x, y=y)) + 
  ggtitle("OLS Regression Line") +
  geom_point(size = 0.05, color = "black", alpha = 0.5) +
  geom_smooth(method = lm, color = "black") +
  annotate("text", x = -1.5, y = 30, color = "red", 
           label = paste("Intercept = ", -0.0732608)) +
  annotate("text", x = 1.5, y = -30, color = "blue", 
           label = paste("Slope =", 5.685033)) + 
  theme_classic() + labs(x="X", y="Y")
```

### Measures of Regression performance

Let's calculate the R-squared by hand first:

```{r r2-manual}
SST <- tb %>% mutate(ybar=mean(y), sst=(y-ybar)^2) %>% select(sst) %>% summarise(sum(sst))
SSE <- tb %>% mutate(ybar=mean(y), sse=(yhat1-ybar)^2) %>% select(sse) %>% summarise(sum(sse))
SSR <- tb %>% mutate(ssr=uhat1^2) %>% select(ssr) %>% summarise(sum(ssr))

R2 <- SSE/SST
print(paste0("The R-squared is ", round(as.numeric(R2)*100,digits=2), "%", sep=""))
```

You can also retrieve it straight from the lm command (this is almost always true for whatever regression object you might want!)

```{r r2}
summary(reg_tb) # Full summary
round(summary(reg_tb)$r.squared,digits=4) # Specifically, the r-squared
```

### Unbiasedness of our regressions

We've already shown that our model does reasonably well matching the truth, but can we show that in general? Let's try another simulation.

First, let's start with a population model (which we assume we know, in order to show that our regression can approach it):

$$ y = 3 + 2x + \varepsilon.$$ We will assume that $x$ and $\varepsilon$ are normally distributed, with $x \sim \mathcal{N}(0,9)$ and $\varepsilon \sim \mathcal{N}(0,36)$. We assume all the standard regression assumptions are met, so that $x$ and $\varepsilon$ are independent.

Let's use this DGP to create a random sample of data ($N$=10,000) and run a regression. We will repeat this procedure 1,000 times, storing the regression coefficients each time.

```{r unbiasedness}
lm <- lapply( # lapply is a useful command (see ?lapply to access documentation)
  1:1000,
  function(x) tibble(
    x = 9*rnorm(10000), # Distribution for x (rnorm(N) creates N random draws from a N(0,1) distribution)
    u = 36*rnorm(10000), # Distribution for e
    y = 3 + 2*x + u
  ) %>% 
    lm(y ~ x, .)
) # This runs 1,000 regressions with new data every time
# lapply is useful here since we want to resample data *and* do a regression at the same time. 
```

What are the results of this simulation? Let's make a table and a graph.

```{r unbiasedness-results}
as_tibble(t(sapply(lm, coef))) %>%
  summary(x) # Summarize the regression coefficients (tabular)

as_tibble(t(sapply(lm, coef))) %>% 
  ggplot()+
  geom_histogram(aes(x), binwidth = 0.01,fill="gray",color='black') + 
  theme_minimal() + 
  labs(x="Estimated Slope Coefficient", y="Count",title="Distribution of Slope Coefficients") + 
  geom_vline(xintercept=2,color='red')
```


## Control Variables

First, let's explore the relationship between A1C checks and hospitalizations. We'll use some fake data for this, so don't trust any of the results you see reported here! 

```{r fake-data,echo=F}
# Create a data set of 1000 physicians, with some rates of hospitalizations, A1Cs, and average patient education (in years)

covmat <- matrix(c(1,.5,-.2,.5,1,.5,-.2,.5,1),nrow=3,ncol=3) # Covariance matrix used to simulate the data
mydata <- rnorm_multi(1000,3,0,1,covmat,varnames=c("hospitalizations","a1cs","education"))
view(mydata) # this command lets us look at the data (but we don't want it to print out into the rmd, hence the echo=F)
```

### How do controls change the results?

If we don't include controls, we get a simple regression of just the effect of A1Cs on hospitalizations: 
```{r simple}
lm_simple <- lm(hospitalizations ~ a1cs, data=mydata)
summary(lm_simple)
```

But if we control for average patient education, the results change: 

```{r control-education}
# Now, control for education 
lm_complex <- lm(hospitalizations ~ a1cs + education, data=mydata)
summary(lm_complex)
```
## Dummy Variables

Let's add another control in the form of a dummy variable: physician sex (we want this to be binary for the purposes of the regression, so we will use sex instead of gender identity).

```{r unorganized}
mydata <- mydata %>% mutate(female = ifelse(runif(nrow(mydata))>0.6, 1, 0),
                            male = 1-female)

# What is the gender composition of our sample?
summary(mydata$female)
```

Now let's include this dummy variable in the regression, and compare all three results in a regression table using modelsummary 

```{r dummy-reg}
# Linear model with dummy variable included
lm_dummy <- lm(hospitalizations ~ a1cs + education + female, data=mydata)
summary(lm_dummy) # What does it mean that this is significant? Should it be? 
  # Return to this at the end of the lecture when we talk about validity traps

# Let's make a regression table for ease of interpretation
msummary(list("Simple"=lm_simple,"One Control"=lm_complex,"Full"=lm_dummy),
         stars=c('*' = .1, '**' = .05, '***' = .01))
```

### Dummy Variable Trap

What happens if we include both $1\{male\}$ and $1\{female\}$ in a regression? 

```{r dvt}
lm_dummytrap <- lm(hospitalizations ~ a1cs + education + female + male, data=mydata)
summary(lm_dummytrap)
  # Code can usually detect the dummy variable trap, but don't count on it! 
```

### Multiple dummy variables

Finally, how does this change if we have multiple dummy variables in a set? Let's consider the region in which each physician practices. 

```{r region} 
mydata <- mydata %>% mutate(region = sample(seq(1:4),size=nrow(mydata),replace=TRUE)) 
  # Generate a random region variable for restaurants 

# Suppose that 1=West, 2=Midwest, 3=South, 4=East
# Need to generate three dummy variables for the regression 
mydata <- mydata %>% mutate(region_west = (region == 1), 
                      region_midwest = (region == 2), 
                      region_south = (region == 3))
lm_region <- lm(hospitalizations ~ a1cs + education + female + region_west + region_midwest + region_south, data=mydata)
msummary(list("No Regions"=lm_dummy, "With Region Controls"=lm_region),
         stars=c('*' = .1, '**' = .05, '***' = .01)) 
  # Why aren't our other coefficients changed? Thoughts? 

# # Joint test of significance -- helpful to consider after lecture
# library('lmtest')
# lmtest::waldtest(lm_dummy, lm_region) # Run a version of the regression without the variables you want to test, 
# # and one with full set of coefficients. Then use waldtest to test difference
```

## Interaction Terms

Our question will be: what is the effect of education on wages? We can estimate a simple regression controlling only for education and experience -- for now, assume that this is a fully specified model.

```{r mydata}
mydata <- causaldata::close_college

# First pass: simple regression in levels
mydata <- mydata %>% mutate(wage = exp(lwage))
m_levels <- lm(wage ~ educ + exper, data=mydata)

m1 <- lm(lwage ~ educ + exper, data=mydata)
msummary(list("levels"=m_levels, "log_y"=m1),
         stars=c('*' = .1, '**' = .05, '***' = .01)) 
```

But what if we think the returns to education will differ across race? Let's use an interaction model to examine heterogeneous treatment effects here. Recall that in an interaction model we need to include **all level terms** of our variables of interest, not just the interactions.

```{r interactions}
# How does the effect of education differ across race?
m2 <- lm(lwage ~ educ + exper + black + educ:black, data=mydata)
msummary(list(m1,m2),
         stars=c('*' = .1, '**' = .05, '***' = .01)) 

# What if we had left out the main effect?
m3 <- lm(lwage ~ educ + exper + educ:black, data=mydata)
msummary(list(m2,m3),
         stars=c('*' = .1, '**' = .05, '***' = .01)) 
```


## Standard Errors 

### How to obtain standard errors
Just like a lot of other things, standard errors are stored in the lm object! 

```{r ses}
myout <- summary(lm_complex)
myout$coefficients[,2]
```

### Heteroskedasticity

Let's check if our assumption of homoskedasticity is satisfied.

```{r plot-variance}
mydata <- mydata %>% mutate(wage=exp(lwage))
ggplot(mydata,aes(x=educ,y=wage)) + geom_point() + theme_minimal()
```

Clearly, this assumption fails. Let's implement robust standard errors in our regressions

```{r robust-ses}
m1 <- lm(lwage ~ educ + exper + black + married + nearc4 + educ:black + exper:black, data=mydata)
msummary(list("naive"=m1),
         stars=c('*' = .1, '**' = .05, '***' = .01))

# Same model with robust standard errors
msummary(list("naive"=m1,"robust"=m1),
         vcov=c("classical","robust"),
         stars=c('*' = .1, '**' = .05, '***' = .01))
```

### Clustering

Now, suppose that we have data on geography (US State). What if the returns to education/experience are correlated within states in unique ways? Why might this be true?

```{r clustered-ses}
mydata$state <- unlist(causaldata::abortion[1:3010,1])
m1 <- lm(lwage ~ educ + exper + black + married + nearc4 + educ:black + exper:black, data=mydata)

msummary(list("naive"=m1,"robust"=m1,"Clustered"=m1),
         vcov=c("classical","robust",~state),
         stars=c('*' = .1, '**' = .05, '***' = .01))
```

### Bootstrapping standard errors

For whatever reason, you may need to bootstrap your standard errors. This is a sample code for how you might do that.

```{r bootstrap, eval=FALSE}
numiter <- 1000 # Number of times you want to sample
sizesample=nrow(mydata) # Size of the sample you want to take

# Create a place to store the coefficients
allbetas <- rep(NA,numiter)

# Run the loop
for (i in 1:numiter) { 
  sampledata <- mydata[sample(nrow(mydata), sizesample,replace=T), ]
  samplemodel <- lm(lwage ~ educ + exper + black + married + nearc4 + educ:black + exper:black, data=sampledata)
  allbetas[i] <- samplemodel$coefficients[2] # Effect of education on lwage
}

# Represent graphically
allbetas <- tibble(allbetas)
ggplot(allbetas,aes(x=allbetas)) + geom_histogram(fill='gray',color="black") + theme_minimal() + 
  labs(x="Estimated Betas",y="Count") + geom_vline(xintercept=0.077,color="red",size=2)

sd(allbetas$allbetas) # This gives us a bootstrapped SE remarkably similar to the regression approach
```

## Making a Regression Table

### Preliminaries 

When making and saving outputs, you want your code (and outputs) to be **completely replicable**. You will absolutely have to make these figures and tables more times than you think you should reasonably have to! 

So make sure you have: 
1. An R project set up
2. A command that points R to a directory **regardless of your machine** 
    a. This is where the "here" package comes in handy
3. Consistent project organization (folders) with dates for each replication of a figure

```{r reg-tables}
# Make sure your directory is working
library(here)
here() # Note: this will be the directory your project is stored in

# Some sample data for our table construction
res <- causaldata::restaurant_inspections

res <- res %>%
  # Create NumberofLocations
  group_by(business_name) %>%
  mutate(NumberofLocations = n())
summary(res$NumberofLocations)

## Let's run our regression models
m1 <- lm(inspection_score ~ NumberofLocations, data = res)
m2 <- lm(inspection_score ~ NumberofLocations + Year, data = res)

# Give msummary a list() of the models we want in our table
# and save to a file using the here() library
# Note that you select the file type (html, pdf, etc.)
# (see help(msummary) for other options)
msummary(list("OLS, unclustered SEs"=m1,"Full"=m2),
         stars=TRUE) # ,
         output= here('regression_table.html')) 
# If you want a folder within your directory, you would say here("Output", "RegressionTable.html")

# Default significance stars are +/*/**/*** .1/.05/.01/.001. Social science
# standard */**/*** .1/.05/.01 can be restored with stars=c('*' = .1, '**' = .05, '***' = .01)
```

## Package Citations
```{r, include=FALSE}
print("=============================Works Cited=============================")
loadedNamespaces() %>%
map(citation) %>%
print(style = "text") # Adds citations for each package to end of .rmd file

knitr::write_bib(file = 'packages.bib') # Constructs a citation file for all packages used in this lecture.

# DON'T FORGET TO CITE YOUR PACKAGES IN YOUR PAPERS/ASSIGNMENTS.
```
Let's knit this file and save it!