---
title: "Lecture 3 Code"
author: "Alex Hoagland"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r header,echo=FALSE, results='hide'}
# I like to include several additional notes in the header of my files here:
#
# Last modified: 10/4/2023
#
### PURPOSE:
  # Lecture 3 code and output file
# 
### NOTES:
  # - uses the Tidyverse package and Dplyr
  # - uses the NHANES package to load data from the US National Health and Nutrition Examination Survey (NHANES, 1999-2004). 

library(tidyverse)
library(wooldridge)
# install.packages("pwr")
library(pwr)
# install.packages("pwrss")
library(pwrss)
```

## Simple hypothesis test: One mean

Let's start with defining the test procedure for a simple mean. We will use the Wooldridge `wagepan` data set.

```{r simple-test}
# Load the Wooldridge wag2 data
mydata <- wooldridge::wagepan

summary(mydata) # What are we looking at? 

# Let's describe wages
mydata %>% mutate(wage = exp(lwage)) %>% # convert to dollars
  ggplot() + geom_histogram(aes(wage),
                            binwidth=1,fill='skyblue',color='black',alpha=.8) +
  theme_classic() + 
  geom_vline(xintercept=5.9192, color="red", linetype="dashed") +
    # line for average wage in sample
  labs(x="Hourly Wages", y="") # plot the distribution of wages

# What do we see here? 

# Let's test the hypothesis that the average wage is $7.77 (average hourly wage in USD, 1985)
  # First, what's the null here? Why did we choose it? 
  # What's the test statistic?
  mean(exp(mydata$lwage)) # average wage is $5.92 in sample
  sd(exp(mydata$lwage)) # SD is 3.20
  (5.92-7.77)/(3.20/sqrt(4360)) # t-statistic (how do we interpret this?)
t.test(exp(mydata$lwage), mu=7.77) # What do we think? Ignore p-value for now

  # What's the conclusion? How do we interpret our test? 
```

## Testing difference in means

Okay, now suppose we want to test difference in wages across groups. Let's look at two comparisons: black/non-black and health status.

```{r test-differences}
# First, let's compare black/non-black. 
# Make a bar chart showing size of black and non-black groups in mydata
mydata %>% ggplot() + geom_bar(aes(black), fill='palegreen2',color='black',alpha=.8) +
  theme_classic() + labs(x="Black", y="Count") 
summary(mydata$black) # What do we make of this? 

# Now make a two-way histogram for exp(lwage) across black and non-black in mydata
mydata %>% mutate(wage = exp(lwage)) %>% ggplot() + geom_histogram(aes(wage,fill=as.factor(black)),                           binwidth=1,position="dodge",color='black',alpha=.8) +
  theme_classic() + labs(x="Hourly Wages", y="",fill="Black") + facet_wrap(~black) # plot the distribution of wages

# Now let's test the difference in wages across black and non-black
  # First, what's the null here? Why did we choose it? 
  # What's the test statistic?
  mean(exp(mydata[which(mydata$black==0),]$lwage)) # 6.00
  mean(exp(mydata[which(mydata$black==1),]$lwage)) # 5.26
  sd(exp(mydata[which(mydata$black==0),]$lwage)) # SD is 3.25
  sd(exp(mydata[which(mydata$black==1),]$lwage)) # SD is 2.72
  (6.00-5.26)/(sqrt((3.25^2/3856)+(2.72^2/504))) # t-statistic (how do we interpret this?)
t.test(exp(mydata$lwage) ~ mydata$black) # What do we think?

# What if we do the same thing for poor health instead of black? 
# Make a bar chart showing size of groups
mydata %>% ggplot() + geom_bar(aes(poorhlth), fill='royalblue4',color='black',alpha=.8) +
  theme_classic() + labs(x="Poor Health", y="Count") 
summary(mydata$poorhlth) # What do we make of this? 

# Now make a two-way histogram for exp(lwage) across groups
mydata %>% mutate(wage = exp(lwage)) %>% ggplot() + geom_histogram(aes(wage,fill=as.factor(poorhlth)),                           binwidth=1,position="dodge",color='black',alpha=.8) +
  theme_classic() + labs(x="Hourly Wages", y="",fill="Poor Health") + facet_wrap(~poorhlth) # plot the distribution of wages

# Now let's test the difference in wages across groups
  # First, what's the null here? Why did we choose it? 
  # What's the test statistic?
  mean(exp(mydata[which(mydata$poorhlth==0),]$lwage)) # 5.93
  mean(exp(mydata[which(mydata$poorhlth==1),]$lwage)) # 5.21
  sd(exp(mydata[which(mydata$poorhlth==0),]$lwage)) # SD is 3.21
  sd(exp(mydata[which(mydata$poorhlth==1),]$lwage)) # SD is 2.60
  (5.93-5.21)/(sqrt((3.21^2/(4360-74))+(2.60^2/74))) # t-statistic (how do we interpret this?)
t.test(exp(mydata$lwage) ~ mydata$poorhlth) # What do we think?
```

## Examining rejection regions
Let's look more carefully at rejection regions 

```{r rejection-regions}
area_poly <- function(cur, cutoff, side=c(1,-1), col = "grey", border=NA, ...)
{
  if (side[1]>0 )# on the right
  {
    pos <- min(which(cur$x > cutoff))
    end <- length(cur$x)
  }
  else # on the left
  {
    pos <- max(which(cur$x < cutoff))
    end <- 1
  }
  polygon(x=c(cur$x[end:pos], cur$x[pos], cur$x[end]),
          y=c(cur$y[end:pos], 0, 0), col=col, border=border, ...)
}

# Assign the curve cc (here, we don't yet standardize)
cc <- curve(dnorm(x, mean = 20, sd = 10), from = -10, to = 50, n = 1000, lwd = 3, xlab = "", ylab = "Density", frame = F)
area_poly(cc, cutoff = 16, side = 1, col = "grey50", density = 10)

# What if we standardize?
cc <- curve(dnorm(x, mean = 0, sd = 1), from = -4, to = 4, n = 1000, lwd = 3, xlab = "", ylab = "Density", frame = F)
area_poly(cc, cutoff = -.4, side = 1, col = "grey50", density = 10)

# But we haven't yet adjusted for standard error!
cc <- curve(dnorm(x, mean = 0, sd = 1), from = -4, to = 4, n = 1000, lwd = 3, xlab = "", ylab = "Density", frame = F)
area_poly(cc, cutoff = -.4*sqrt(100), side = 1, col = "grey50", density = 10)
```

Specifically, we have to decide if we want a one-sided or a two-sided test. 

``` {r one-two-sided-test}
# What about a two-sided test? 
# plot the standard normal density on the interval [-4,4]
curve(dnorm(x),
      xlim = c(-4, 4),
      main = 'Calculating a p-Value',
      yaxs = 'i',
      xlab = 'z',
      ylab = '',
      lwd = 2,
      axes = 'F')

# add x-axis
axis(1, 
     at = c(-1.5, 0, 1.5), 
     padj = 0.75,
     labels = c(expression(-frac(bar(x)~-~bar(mu)[x,0], sigma[bar(x)])),
                0,
                expression(frac(bar(x)~-~bar(mu)[x,0], sigma[bar(x)]))))

# shade p-value/2 region in left tail
polygon(x = c(-6, seq(-6, -1.5, 0.01), -1.5),
        y = c(0, dnorm(seq(-6, -1.5, 0.01)),0), 
        col = 'steelblue')

# shade p-value/2 region in right tail
polygon(x = c(1.5, seq(1.5, 6, 0.01), 6),
        y = c(0, dnorm(seq(1.5, 6, 0.01)), 0), 
        col = 'steelblue')

# What about a ONE-sided test? 
# plot the standard normal density on the interval [-4,4]
curve(dnorm(x),
      xlim = c(-4, 4),
      main = 'Calculating a p-Value',
      yaxs = 'i',
      xlab = 'z',
      ylab = '',
      lwd = 2,
      axes = 'F')

# add x-axis
axis(1, 
     at = c(-.5, 0.5), 
     padj = 0.75,
     labels = c(expression(-frac(bar(x)~-~bar(mu)[x,0], sigma[bar(x)])),
                0))

# shade p-value region all in left tail
polygon(x = c(-6, seq(-6, -.5, 0.01), -.5),
        y = c(0, dnorm(seq(-6, -.5, 0.01)),0), 
        col = 'steelblue')
```

### Why can we use the CLT here? 
The Central Limit Theorem (CLT) states that the sampling distribution of the sample mean is approximately normally distributed, regardless of the distribution of the underlying population, as long as the sample size is sufficiently large.

Importantly, the test statistic here also follows a standard normal as the sample size increases. We can show this through simulating. 

```{r simulation-t}
# prepare empty vector for t-statistics
tstatistics <- numeric(10000)

# set sample size
n <- 300 # play around with this parameter

# simulate 10000 t-statistics
for (i in 1:10000) {
  
  s <- sample(0:1, 
              size = n,  
              prob = c(0.9, 0.1),
              replace = T)
  
  tstatistics[i] <- (mean(s)-0.1)/sqrt(var(s)/n)
  
}

# plot density and compare to N(0,1) density
plot(density(tstatistics),
     xlab = 't-statistic',
     main = 'Estimated Distribution of the t-statistic when n=300',
     lwd = 2,
     xlim = c(-4, 4),
     col = 'steelblue')

# N(0,1) density (dashed)
curve(dnorm(x), 
      add = T, 
      lty = 2, 
      lwd = 2) # so think carefully -- what is the distribution we're showing here? 
```

### How does sample size influence testing outcomes? 
Finally, let's use the example in the slides about tipping to explore the effect of limited/expanded sample sizes on testing outcomes

```{r small-n-tests}
# set seed for reproducibility
set.seed(0813)

# sample data that has 16 observations with mean 12 and var 23
tip_data <- data.frame(rnorm(16, mean = 12, sd = sqrt(23)))
names(tip_data) <- "tips"

# can we reject the null hypothesis of mean = 15 with 99% confidence? 
t.test(tip_data, mu = 15, conf.level = 0.99)
confint(lm(tips~1,tip_data), level=0.99) # 99% confidence interval for tip_data

# Okay, suppose that we had a larger sample size of n=100
tip_data <- data.frame(rnorm(100, mean = 12, sd = sqrt(23)))
names(tip_data) <- "tips"
t.test(tip_data, mu = 15, conf.level = 0.99)
confint(lm(tips~1,tip_data), level=0.99) 

# What if variance changed too? 
tip_data <- data.frame(rnorm(100, mean = 12, sd = sqrt(100)))
names(tip_data) <- "tips"
t.test(tip_data, mu = 15, conf.level = 0.99)
confint(lm(tips~1,tip_data), level=0.99) 
```

### Tests for proportions
Finally, let's implement a test for proportions using the wagepan dataset. 

```{r prop-test}
# We will test whether the fraction of unionized workers is different from 1980 to 1987
summary(mydata[which(mydata$year==1980),]$union)
summary(mydata[which(mydata$year==1987),]$union) # do we think it will be? 

# To test this, let's trim the data
testdata <- mydata %>% filter(year %in% c(1980, 1987)) %>% select(union, year)
t.test(testdata$union ~ testdata$year, var.equal = F, conf.level = 0.95)
```

## P-values
Let's go back to the example from last time about tipping and think about the p-values here. 

```{r small-n-tests-pvalues}
# set seed for reproducibility
set.seed(0813)

# sample data that has 16 observations with mean 12 and var 23
tip_data <- data.frame(rnorm(16, mean = 12, sd = sqrt(23)))
names(tip_data) <- "tips"

# can we reject the null hypothesis of mean = 15 with 99% confidence? 
t.test(tip_data, mu = 15, conf.level = 0.99)
confint(lm(tips~1,tip_data), level=0.99) # 99% confidence interval for tip_data

# Okay, suppose that we had a larger sample size of n=100
tip_data <- data.frame(rnorm(100, mean = 12, sd = sqrt(23)))
names(tip_data) <- "tips"
t.test(tip_data, mu = 15, conf.level = 0.99)
confint(lm(tips~1,tip_data), level=0.99) 

# What if variance changed too? 
tip_data <- data.frame(rnorm(100, mean = 12, sd = sqrt(100)))
names(tip_data) <- "tips"
t.test(tip_data, mu = 15, conf.level = 0.99)
confint(lm(tips~1,tip_data), level=0.99) 
```

### Distributions of P-values
We can show these simulations directly in R. 

```{r pvalue-sim-true}
# The null hypothesis is that the average is equal to 0
# We will simulate 10000 p-values under the null hypothesis
pvalues <- numeric(10000)
for (i in 1:10000) {
  s <- rnorm(100,mean=0,sd=100)
  pvalues[i] <- t.test(s, mu = 0)$p.value
}
hist(pvalues)
```

```{r pvalue-sim-false}
# The null hypothesis is that the average is equal to 0
# We will simulate 10000 p-values under a slightly false null hypothesis
pvalues <- numeric(10000)
for (i in 1:10000) {
  s <- rnorm(100,mean=10,sd=100)
  pvalues[i] <- t.test(s, mu = 0)$p.value
}
hist(pvalues)

# We will simulate 10000 p-values under a wildly false null hypothesis
pvalues <- numeric(10000)
for (i in 1:10000) {
  s <- rnorm(100,mean=100,sd=100)
  pvalues[i] <- t.test(s, mu = 0)$p.value
}
hist(pvalues)
```

### Relationship between p-values and sample size. 
Finally, let's show the relationship between p-values and sample size **for the same test**.

```{r pvalue-sim-samplesize}
# The null hypothesis is that the average is equal to 100
# We will construct data with a true mean of 101 and different sample sizes
makedata <- function(n=100) { 
  testdata <- data.frame(rnorm(n, mean = 101, sd = 10))
  names(testdata) <- "sample"
  return(testdata)
}
mydata <- makedata(10000)
hist(mydata$sample)

# Now let's look at how p-values vary with sample size
 t.test(makedata(25), mu = 100)$p.value
 t.test(makedata(100), mu = 100)$p.value
 t.test(makedata(400), mu = 100)$p.value
 t.test(makedata(900), mu = 100)$p.value
 t.test(makedata(2500), mu = 100)$p.value
 t.test(makedata(10000), mu = 100)$p.value
 t.test(makedata(1e7), mu = 100)$p.value # is it wild to think of having a million observations? 

```

## Power

### What is power? 
Let's visualize what we're talking about 

```{r power-calcs,warning=FALSE}
power.z.test(ncp = 1.96, # critical value
             alpha = 0.05, # pr(type 1 error)
             alternative = "not equal", # what kind of test?
             plot = TRUE)
```

### Power calculations visualized
We can visualize the statistical power we would need under different hypothesized correlations. 

```{r power-calcs-3}
# Plot sample size curves for detecting correlations of
# various sizes.

# range of correlations
r <- seq(.1,.5,.01)
nr <- length(r)

# power values
p <- seq(.4,.9,.1)
np <- length(p)

# obtain sample sizes
samsize <- array(numeric(nr*np), dim=c(nr,np))
for (i in 1:np){
  for (j in 1:nr){
    result <- pwr.r.test(n = NULL, r = r[j],
    sig.level = .05, power = p[i],
    alternative = "two.sided")
    samsize[j,i] <- ceiling(result$n)
  }
}

# set up graph
xrange <- range(r)
yrange <- round(range(samsize))
colors <- rainbow(length(p))
plot(xrange, yrange, type="n",
  xlab="Correlation Coefficient (r)",
  ylab="Sample Size (n)" )

# add power curves
for (i in 1:np){
  lines(r, samsize[,i], type="l", lwd=2, col=colors[i])
}

# add annotation (grid lines, title, legend)
abline(v=0, h=seq(0,yrange[2],50), lty=2, col="grey89")
abline(h=0, v=seq(xrange[1],xrange[2],.02), lty=2,
   col="grey89")
title("Sample Size Estimation for Correlation Studies\n
  Sig=0.05 (Two-tailed)")
legend("topright", title="Power",
as.character(p),
   fill=colors)
```

### Power calculations in R
How do we calculate power directly? 

```{r power-calcs-2}
# You must leave exactly one out of the following: 
#      n, d (effect size), power, and sig.level
# Note that d = difference in means divided by pooled standard deviation
pwr.t.test(n=NULL, d = 0.1, sig.level = 0.01, power = 0.9, 
           type = "two.sample", alternative = "two.sided")
```

## Package Citations

```{r, include=FALSE}
print("=============================Works Cited=============================")
loadedNamespaces() %>%
map(citation) %>%
print(style = "text") # Adds citations for each package to end of .rmd file

knitr::write_bib(file = 'packages.bib') # Constructs a citation file for all packages used in this lecture.

# DON'T FORGET TO CITE YOUR PACKAGES IN YOUR PAPERS/ASSIGNMENTS. 
```

Let's knit this file and save it!
