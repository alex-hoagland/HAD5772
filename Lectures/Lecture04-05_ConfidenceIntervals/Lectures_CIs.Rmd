---
title: "Lecture 3 Code"
author: "Alex Hoagland"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r header}
# I like to include several additional notes in the header of my files here:
#
# Last modified: 2024-01-23
#
### PURPOSE:
  # Lecture 3 code and output file
#
### NOTES:
  # - uses the Tidyverse package and Dplyr
library(tidyverse)
```

## Calculating values from the normal distribution
Note: this is all very similar across the "main" distributions that R knows. We just use the normal distribution here as an example (*todo*: add an example here using copilot that samples from the Bernoulli and creates some data)

```{r bernoulli-sample}

# Create a dataset called mydata that samples from a Bernoulli distribution with p = .15 and n = 10000
mydata <- rbinom(n=10000,size=1,prob=.15)

# Loop through 100 iterations of a Bernoulli sampling process with p = .15 and n = 10000, store the mean each time, and create a histogram of the mean
allsamples <- rep(NA, 100) # empty frame to store results
for (i in 1:100) {
  test <- rbinom(n=10000,size=1,prob=.15)
  allsamples[i] <- mean(test)
  rm(test)
}
hist(allsamples)
```

```{r normal-dist}
# If you want to sample from a normal distribution:
sample_normal <- rnorm(n=1000,mean=5,sd=10)
hist(sample_normal) # think: how would you standardize this?

# Standardize sample_normal
sample_normal2 <- (sample_normal - mean(sample_normal))/sd(sample_normal)
sample_normal3 <- (sample_normal-5)/10

# First, pdf
pnorm(q=0,mean=0,sd=1)
pnorm(q=10,mean=5,sd=10) # what is the probability that a normal draw is less than 10?

# Second, cdf
dnorm(x=10,mean=5,sd=10) # this gives the density

# Third, quantiles

# Identify the 5th and 95th percentiles of a normal distribution with mean 5 and sd 10
qnorm(p=c(0.05,0.95),mean=5,sd=10)
qnorm(p=c(.1,.2,.3,.4,.5,.6,.7,.8,.9,1),mean=5,sd=10) # quantiles of the distribution

# Fourth, critical values
qnorm(p=c(.025,.975),mean=5,sd=10) # two-sided quantiles of the distribution
```

## Sampling and standard errors
Let's suppose that we have a population (we don't) from which we repeatedly sample and estimate a mean. For this one, we will use data on COVID prevalence.
```{r sample-data}
# install.packages("medicaldata")
library(medicaldata)

data(package = "medicaldata") # this will show you the full list of data sets
# https://higgi13425.github.io/medicaldata/

# Let's use the covid-testing data
mydata <- covid_testing # Suppose that this is the population

# If we want to take samples of 500 patients each
sample1 <- mydata %>% sample_n(500, replace=TRUE)
sample1 %>% mutate(positive = (result == "positive")*100) %>% # generate new variable (in %)
  ungroup() %>% select(positive) %>% summarize_all(mean) # summarize average value

# What if we take 100 of these samples?
allsamples <- rep(NA, 100) # empty frame to store results
for (i in 1:10) {
 test <- mydata %>% sample_n(500, replace=TRUE)
  allsamples[i] <- test %>% mutate(positive = (result == "positive")*100) %>% # generate new variable (in %)
  ungroup() %>% select(positive) %>% summarize_all(mean) %>% as.numeric() # summarize average value
  rm(test)
}
hist(allsamples)
sd(allsamples) # This is a measure of variability

# What about calculating SEs directly in the pouplation?
mydata <- mydata %>% mutate(positive = (result == "positive")*100)
summary <- mydata %>% select(positive) %>% summarize(mean = mean(positive),
                                                         sd=sd(positive), n=n())
summary$sd/sqrt(summary$n) # this is the standard error for our mean

# note: any concerns with this? technically the test is a binomial random variable, so we would actually need a different way of calculating se
mydata <- mydata %>% mutate(positive_frac = positive/100) # in decimals rather than percents
# we then use the formula for the standard error of a proportion
summary1 <- mydata %>% select(positive_frac) %>% summarize(mean = mean(positive_frac),
                                                         sd=sd(positive_frac), n=n())
sqrt(summary1$mean*(1-summary1$mean)/summary1$n) # note that this isn't very different in this case (need to convert between percentages and decimals).
```

Now, let's talk about *plotting* uncertainty. What if we wanted to plot average positive rate across age bins?
```{r data-viz-uncertainty}
# first, create some age bins
mydata <- mydata %>% mutate(agebin = ifelse(age < 18, 1,
                                            ifelse(age >= 18 & age < 30, 2,
                                                            ifelse(age >= 30 & age < 65, 3, 4))))

# now without uncertainty, how can we plot
ggplot(mydata) +
  geom_bar(aes(as.factor(agebin), positive,fill=as.factor(agebin)),
           position = "dodge", stat = "summary", fun.y = "mean") +
  theme_classic() + labs(x="Age Bin", y="% Positive", fill="") # what do we think here?

# now how to add uncertainty -- error bars
mydata <- mydata %>% group_by(agebin) %>% mutate(mean = mean(positive),  
                                                 sd=sd(positive),      
                                                 se = sd/sqrt(n()))
      # varies within groups)
ggplot(mydata) +
  geom_bar(aes(as.factor(agebin), positive,fill=as.factor(agebin)),
           position = "dodge", stat = "summary", fun.y = "mean") +
  theme_classic() + labs(x="Age Bin", y="% Positive", fill="") +
  geom_errorbar(aes(x=as.factor(agebin), ymin=mean-sd, ymax=mean+sd), width=.2,position=position_dodge(.9)) # What happens if I just add SDs?

ggplot(mydata) +
  geom_bar(aes(as.factor(agebin), positive,fill=as.factor(agebin)),
           position = "dodge", stat = "summary", fun.y = "mean") +
  theme_classic() + labs(x="Age Bin", y="% Positive", fill="") +
  geom_errorbar(aes(x=as.factor(agebin), 
                    ymin=mean-1.96*se, ymax=mean+1.96*se), width=.2,position=position_dodge(.9)) # Now what's the story?
```

## Calculating confidence intervals
Note that we already built a confidence interval! All we need to do is appropriately adjust the standard error in the graph above. So how do we interpret it?

Suppose we want to calculate these directly. Can we? Yes!
```{r ci}
# confidence intervals across agebins
mydata %>%
  group_by(agebin) %>%
  summarise(ci = list(mean_cl_normal(positive) %>%
                        rename(mean=y, lwr=ymin, upr=ymax))) %>%
  unnest(cols=c(ci)) # what do we think about this? How could we present it?

# calculate the 90% confidence interval of positive in mydata
mean_cl_normal(mydata$positive, conf.int = .9) 

# now suppose that we want a different level of confidence
ci_95 <- mydata %>%
  group_by(agebin) %>%
  summarise(ci = list(mean_cl_normal(positive*100) %>%
                        rename(mean=y, lwr=ymin, upr=ymax))) %>%
  unnest(cols=c(ci)) # what do we think about this? How could we present it?
ci_99 <- mydata %>%
  group_by(agebin) %>%
  summarise(ci = list(mean_cl_normal(positive*100,conf.int=.99) %>%
                        rename(mean=y, lwr=ymin, upr=ymax))) %>%
  unnest(cols=c(ci)) # what do we think about this? How could we present it?
```

### Confidence intervals for proportions vs. means
Let's get a little more specific into confidence intervals for proportions vs. means
```{r ci-proportions}
# When we have a binary outcome variable, we can calculate SEs and CIs based on the binomial distribution

# Let's look at how the CIs would be different if we ignored this
# For this example, consider test positivity across groups of "payors" for the test
mydata <- mydata %>% group_by(payor_group) %>%
  mutate(mymean = mean(positive_frac,na.rm=T),
         sd = sd(positive_frac),
         mycount = n())
mydata <- mydata %>% mutate(invmean = 1 - mymean)
mydata <- mydata %>% mutate(se_mean = sd/sqrt(mycount),
                            se_prop = sqrt(mymean*(1-mymean)/mycount), 
                            diff = se_mean-se_prop)

# How different are these? Let's make a histogram of mydata$diff
mydata %>% filter(abs(diff) < 1e-5) %>% 
  ggplot() + geom_histogram(aes(diff),bins=20) + theme_classic() + 
  labs(x="Difference in SEs", y="Count") # Very small!

# Compare the confidence intervals
mydata %>% filter(!is.na(payor_group)) %>% ggplot() +
  geom_bar(aes(as.factor(payor_group), positive_frac,fill=as.factor(payor_group)),
           position = "dodge", stat = "summary", fun = "mean") +
  theme_classic() + labs(x="Payor Group", y="% Positive", fill="") +
  geom_errorbar(aes(x=as.factor(payor_group),
                    ymin=(mymean-1.96*se_mean), ymax=(mymean+1.96*se_mean)),
                width=.2,position=position_dodge(.9)) +
    geom_errorbar(aes(x=as.factor(payor_group),
                    ymin=(mymean-1.96*se_prop), ymax=(mymean+1.96*se_prop)),
                width=.2,position=position_dodge(.9),color='brown') +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1))
# What do we think about this overall, and then comparing the two types of error bars?
```

### CIs for Poisson data
Suppose that we wanted to say something about *how many* tests each individual got, rather than the outcome of tests. That is, what if we were worried about differential access to testing?
```{r poisson}
# to get some fake data on this, let's collapse by first name in the dataset
collapsed <- mydata %>% group_by(fake_first_name) %>% summarize(tests = n())
hist(collapsed$tests) # Look Poisson enough?

summary(collapsed$tests) # note the skew
# but we don't need a package to get SEs!
collapsed %>% ungroup() %>% summarize(mean = mean(tests), n = n()) %>%
  mutate(se = sqrt(mean/n)) %>% select(se)

# Now how can you plot this? Make a CI? Try it!
```

### CIs for differences in means
Finally, let's test for the difference between means/proportions across two groups: men and women. We'll look at a continuous variable (time to first test) and a binary variable (positivity rates)

```{r diff-means}
# First, let's do a difference in means
t.test(pan_day ~ gender, data = mydata, var.equal = FALSE)

# Second, let's do a difference in proportions
# install.packages('tidymodels') # note: this takes a while
library(tidymodels)
testdata <- mydata %>% ungroup() %>% select(positive_frac, gender) %>% 
  mutate(positive_frac = as.factor(positive_frac), 
        gender = as.factor(gender))
# prop_test(testdata,detailed=TRUE)
```


## Survival Curves
Let's plot how long people "last" before a test across groups (let's go back to agebins). We can use the "pan_day" variable here since it already has a helpful time 0.

```{r survival}
# Need some packages
# install.packages(c("survival", "survminer"))
library("survival")
library("survminer")

# Need to define a "time 0"
hist(mydata$pan_day)

# Now we can do survival based on bins
fit <- survfit(Surv(pan_day) ~ agebin, data = mydata)
print(fit) # What do we take from this?

# Let's plot!
ggsurvplot(fit,
          pval = TRUE, conf.int = TRUE,
          risk.table = TRUE, # Add risk table
          risk.table.col = "strata", # Change risk table color by groups
          linetype = "strata", # Change line type by groups
          surv.median.line = "hv", # Specify median survival
          ggtheme = theme_bw(), # Change ggplot2 theme
          palette = c("#E7B800", "#2E9FDF", "palegreen", "mediumpurple1"))

# how do we make sense of this?
```

## Package Citations
```{r, include=FALSE}
print("=============================Works Cited=============================")
loadedNamespaces() %>%
map(citation) %>%
print(style = "text") # Adds citations for each package to end of .rmd file

knitr::write_bib(file = 'packages.bib') # Constructs a citation file for all packages used in this lecture.

# DON'T FORGET TO CITE YOUR PACKAGES IN YOUR PAPERS/ASSIGNMENTS.
```
Let's knit this file and save it!