---
title: "Lecture 3 Code"
author: "Alex Hoagland"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r header}
# I like to include several additional notes in the header of my files here:
#
# Last modified: 2024-01-23
#
### PURPOSE:
  # Lecture 3 code and output file
#
### NOTES:
  # - uses the Tidyverse package and Dplyr
library(tidyverse)
library(NHANES)
```

### Covariance and Correlation

```{r covariance-correlation}
mydata <- NHANES
cov(mydata$DaysMentHlthBad, mydata$Age,use="pairwise.complete.obs") # Base covariance -- but in units of what?
cov(mydata$DaysMentHlthBad/7, mydata$Age,use="pairwise.complete.obs")

cor(mydata$DaysMentHlthBad, mydata$Age,use="pairwise.complete.obs") # This makes more sense!
cor(mydata$DaysMentHlthBad/7, mydata$Age,use="pairwise.complete.obs") # This makes more sense!

# What makes this number useful for you? What would you think about changing to make this number more useful, given our results above? 
```

# Random data

When working with a sample of data, we want to understand something about the *randomness* of that data. Hence, we talk about sampling distributions.

## Example: Sampling from a Bernoulli distribution

Just as a simple example, suppose that we have data with two possible outcomes: poor mental health day or not. How can we express this distribution simply?

```{r simple-bernoulli}

# What does this mean? 
set.seed(5772)

# First, sample some data 
sampledata <- rbinom(n=1000,size=1,prob=0.5)
hist(sampledata) # What does this tell us? What should I change above to change the shape of the graph? 

# Second, how can we go from the data we have to an indicator of the probability of poor mental health p?
hist(mydata$DaysMentHlthBad)
mydata %>% mutate(anybad = ifelse(DaysMentHlthBad > 0 & !is.na(DaysMentHlthBad), 
                                  1, 0)) %>% 
  ggplot(aes(x=anybad)) + 
  geom_histogram(aes(y = (..count..)/sum(..count..))) # Does this look like what we had above?

```

So if we had to use this data, what would our estimate of $p = Pr(Poor Mental Health Days > 0)$ be?

## Now what about Binomial? 
We simplified our data a bit above. But what if we wanted to know about the number of poor mental health days in a month (intensive margin) rather than just the probability of poor mental health at all (extensive margin)?

```{r binomial}
# Let's calculate the empirical average successes/fails per person
avgsuccess <- mean(mydata$DaysMentHlthBad, na.rm=T)
avgfails <- mean(30-mydata$DaysMentHlthBad, na.rm=T)
p <- avgsuccess/30 # This is the average # of poor mental health days in a month

hist(mydata$DaysMentHlthBad) # Can we match this to a binomial distribution?
sampledata <- rbinom(n=100000,size=30,prob=p)
hist(sampledata) # What does this tell us? Does our data follow a binomial distribution? 
```

How could we test this empirically? Let's sample the predicted probabilities on just the extensive margin and see how they converge over multiple random samples: 

```{r bernoulli-sample}
# Let's sample, with replacement, from our dataset. 
# Each time we sample, we will compute p = the average Pr(Any Poor Mental Health)
# Then we will do this 100,000 times
# And look at the distribution of estimated p's. 

mydata <- mydata %>% mutate(anybad = ifelse(DaysMentHlthBad > 0 & !is.na(DaysMentHlthBad), 
                                  1, 0))
allsamples <- rep(NA, 1e5) # empty frame to store results
for (i in 1:1e5) {
  test <- sample(mydata$anybad, replace=TRUE, size=1000) # 10% random sample
  allsamples[i] <- mean(test)
  # rm(test)
}
hist(allsamples) # What distribution do we have here? Why? 

# loop 1000 times and take a random sample of 10% of mydata each time. Then within each sample, calculate the average value of anybad and store it in a vector called allsamples. 

```

```{r bernoulli-sample2}
# Now we can calculate the mean and standard deviation of allsamples


# This gives us two things: the estimated average value and the *uncertainty* around that value
mean(allsamples)
sd(allsamples)

standard_allsamples <- (allsamples - mean(allsamples))/sd(allsamples)
pnorm(0.50, mean(allsamples), sd(allsamples))
```

## Calculating values from the normal distribution

Note: this is all very similar across the "main" distributions that R knows. We just use the normal distribution here as an example, but you can use Uniform, Bernoulli, Binomial, Exponential, Gamma, or any other distribution in basically the same way!

```{r normal-dist}
# If you want to sample from a normal distribution:
sample_normal <- rnorm(n=1000,mean=5,sd=10)
hist(sample_normal) # think: how would you standardize this? Can we add code here to do that?
hist(sample_normal2)

# Standardize sample_normal
sample_normal2 <- (sample_normal - mean(sample_normal))/sd(sample_normal)
sample_normal3 <- (sample_normal-5)/10

# First, pdf
pnorm(q=0,mean=0,sd=1)
pnorm(q=10,mean=5,sd=10) # what is the probability that a normal draw is less than 10?

# Second, cdf
dnorm(x=10,mean=5,sd=10) # this gives the density

# Third, quantiles

# Identify the 5th and 95th percentiles of a normal distribution with mean 5 and sd 10
qnorm(p=c(0.05,0.95),mean=5,sd=10)
qnorm(p=c(.1,.2,.3,.4,.5,.6,.7,.8,.9,1),mean=5,sd=10) # quantiles of the distribution

# Fourth, critical values
qnorm(p=c(.025,.975),mean=5,sd=10) # two-sided quantiles of the distribution
```

## Sampling and standard errors

Let's suppose that we have a population (we don't) from which we repeatedly sample and estimate a mean. For this one, we will use data on COVID prevalence.

```{r sample-data}
# install.packages("medicaldata")
library(medicaldata)

data(package = "medicaldata") # this will show you the full list of data sets
# https://higgi13425.github.io/medicaldata/

# Let's use the covid-testing data
mydata <- covid_testing # Suppose that this is the population

# If we want to take samples of 500 patients each
sample1 <- mydata %>% sample_n(500, replace=TRUE)
sample1 %>% mutate(positive = (result == "positive")*100) %>% # generate new variable (in %)
  ungroup() %>% select(positive) %>% summarize_all(mean) # summarize average value

# What if we take 100 of these samples?
allsamples <- rep(NA, 1000) # empty frame to store results
for (i in 1:1000) {
 test <- mydata %>% sample_n(500, replace=TRUE)
  allsamples[i] <- test %>% mutate(positive = (result == "positive")*100) %>% # generate new variable (in %)
  ungroup() %>% select(positive) %>% summarize_all(mean) %>% as.numeric() # summarize average value
  rm(test)
}
hist(allsamples)
sd(allsamples) # This is a measure of variability (how would you think about putting it on the plot?)
```

This isn't that different from the days of poor mental health code above. But what if we wanted to compute standard errors more directly? 

```{r ses-direct}
# What about calculating SEs directly in the pouplation?
mydata <- mydata %>% mutate(positive = (result == "positive"))
se1 <- sd(mydata$positive)/sqrt(nrow(mydata)) # this is the standard error for our mean

# Note: if we actually knew something about the sampling distribution, we could refine these estimates. 
# For examlpe, suppose we know these tests are Bernoulli with p= the average positivity rate (0.06). Then, we can update our SE: 
se2 <- sqrt(0.06*(1-0.06)/nrow(mydata)) # this is the standard error for our mean. Does it differ much? 
```

Now, let's talk about *plotting* uncertainty. What if we wanted to plot average positive rate across age bins?

```{r data-viz-uncertainty}
# first, create some age bins
mydata <- mydata %>% mutate(agebin = ifelse(age < 18, 1,
                                            ifelse(age >= 18 & age < 30, 2,
                                                            ifelse(age >= 30 & age < 65, 3, 4))))

# now without uncertainty, how can we plot?
mydata %>% select(agebin, positive) %>%
  group_by(agebin) %>% summarize(positive= mean(positive)) %>% 
  mutate(agebin = factor(agebin, levels=1:4,labels=c("Under 18", "18-30", "30-64", "65+"))) %>% # make sure this is a factor
  ggplot(aes(x=agebin,y=positive,fill=agebin)) + 
  geom_bar(position = "dodge", stat = "summary") + 
  labs(fill="Age Group", x="", y="Positivity Rate") + theme_minimal() 

# now how to add uncertainty -- error bars
mydata %>% select(agebin, positive) %>%
  group_by(agebin) %>% summarize(mean= mean(positive), sd=sd(positive, na.rm=T), n=n()) %>%
  mutate(se = sd/sqrt(n)) %>% # calculate the standard error
  mutate(agebin = factor(agebin, levels=1:4,labels=c("Under 18", "18-30", "30-64", "65+"))) %>% # make sure this is a factor
  ggplot(aes(x=agebin,y=mean,fill=agebin)) + 
  geom_bar(position = "dodge", stat = "summary") + 
  geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=.2,position=position_dodge(.9)) + 
  labs(fill="Age Group", x="", y="Positivity Rate") + theme_minimal() 
```

The main question, though, is: how much uncertainty should I show? And why? 